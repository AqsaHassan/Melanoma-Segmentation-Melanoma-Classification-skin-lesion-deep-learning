# -*- coding: utf-8 -*-
"""DenseNet2018.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NEMz_ElI4JZ_80h-nJVDag5u2OrtDNW0
"""
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from glob import glob
import seaborn as sns
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from glob import glob
import seaborn as sns
from PIL import Image
from skimage.io import imread
from PIL import Image
from skimage.io import imread
import tensorflow as tf

from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical # convert to one-hot-encoding

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
# %matplotlib inline
# Commented out IPython magic to ensure Python compatibility.
import os
from glob import glob

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical # convert to one-hot-encoding

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras import Model

from keras.preprocessing.image import ImageDataGenerator
from keras import layers
from keras import Model
from keras.applications.densenet import DenseNet201
#from keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam
#from keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import ReduceLROnPlateau
import keras.backend as K

# %matplotlib inline
import matplotlib.pyplot as plt
#from google.colab import drive

#drive.mount('/content/drive')

# Set the ROOT_DIR variable to the root directory of the HAMdataset
#ROOT_DIR = '/content/drive/MyDrive/HAMdataset'
#assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'

#base_skin_dir = "/Users/Hoang/Machine_Learning/skin_cancer/skin-cancer-mnist-ham10000"

base_skin_dir='/home/aqsah/projects/isic2018/isic2018/segment'

skin_df = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata')) # load in the data
skin_df.head()


#DICTIONARY CREATION

imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x
                     for x in glob(os.path.join(base_skin_dir,'SegmentedImage', '*.jpg'))}
print(imageid_path_dict)


lesion_type_dict = {
    'nv': 'Melanocytic_nevi',
    'mel': 'melanoma',
    'bkl': 'Benign_keratosis-like_lesions',
    'bcc': 'Basal_cell_carcinoma',
    'akiec': 'Actinic_keratoses',
    'vasc': 'Vascular_lesions',
    'df': 'Dermatofibroma'
}

lesion_danger = {
    'nv': 0, # 0 for benign
    'mel': 1, # 1 for malignant
    'bkl': 0, # 0 for benign
    'bcc': 1, # 1 for malignant
    'akiec': 1, # 1 for malignant
    'vasc': 0,
    'df': 0
}
print(lesion_danger)


skin_df["path"] = skin_df["image_id"].map(imageid_path_dict.get) # map image_id to the path of that image

skin_df["path"] = skin_df["image_id"].map(imageid_path_dict.get) # map image_id to the path of that image

skin_df["cell_type"] = skin_df["dx"].map(lesion_type_dict.get) # map dx to type of lesion

skin_df["Malignant"] = skin_df["dx"].map(lesion_danger.get)

skin_df.head()

skin_df["cell_type_idx"] = pd.Categorical(skin_df["cell_type"]).codes # give each cell type a category id

skin_df["image"] = skin_df["path"].map(imread)

#Reshape image and get data for classification
reshaped_image = skin_df["path"].map(lambda x: np.asarray(Image.open(x).resize((64,64), resample=Image.LANCZOS).\
                                                          convert("RGB")).ravel())

out_vec = np.stack(reshaped_image, 0)

out_df = pd.DataFrame(out_vec)

out_df["label"] = skin_df["cell_type_idx"]

out_df.head()

out_path = "/home/aqsah/projects/isic2018/isic2018/segment/hmnist_64_64_RBG.csv"
out_df.to_csv(out_path, index=False)

"""Resize Image for Retraining Model"""

img = Image.open(skin_df["path"][0])
img.size

skin_df["cell_type"].unique()

skin_df["path"][0]

for index in skin_df.index.values.tolist():
    path = skin_df.iloc[index]["path"]
    cell_type_idx = skin_df.iloc[index]["cell_type"]
    img_id = skin_df.iloc[index]["image_id"]
    newpath = f"/home/aqsah/projects/isic2018/isic2018/segment/SegmentedImage/{cell_type_idx}/{img_id}.jpg"
    img = Image.open(path)
    img = img.resize((299, 299), resample=Image.LANCZOS)
    img.save(newpath)


"""**Resize Image for Keras Fine-Tuning Model**"""

reshaped_image = skin_df["path"].map(lambda x: np.asarray(Image.open(x).resize((256,192), resample=Image.LANCZOS).\
                                                          convert("RGB")))

out_vec = np.stack(reshaped_image, 0)
out_vec.shape
out_vec = out_vec.astype("float32")
out_vec /= 255

out_vec = out_vec.astype("float32")
out_vec /= 255
labels = skin_df["cell_type_idx"].values
from sklearn.model_selection import train_test_split
X_train_orig, X_test, y_train_orig, y_test = train_test_split(out_vec, labels, test_size=0.1,random_state=0)
np.save("256_192_test.npy", X_test)
np.save("test_labels.npy", y_test)
X_train, X_val, y_train, y_val = train_test_split(X_train_orig, y_train_orig, test_size=0.1, random_state=1)
np.save("256_192_val.npy", X_val)
np.save("val_labels.npy", y_val)
np.save("256_192_train.npy", X_train)
np.save("train_labels.npy", y_train)

X_train = np.load("/home/aqsah/projects/isic2018/isic2018/segment/256_192_train.npy")
y_train = np.load("/home/aqsah/projects/isic2018/isic2018/segment/train_labels.npy")
X_val = np.load("/home/aqsah/projects/isic2018/isic2018/segment/256_192_val.npy")
y_val = np.load("/home/aqsah/projects/isic2018/isic2018/segment/val_labels.npy")
#X_test = np.load("/home/aqsah/projects/isic2018/isic2018/segment/256_192_test.npy")
#y_test = np.load("/home/aqsah/projects/isic2018/isic2018/segment/test_labels.npy")


X_train.shape, X_val.shape

y_train.shape, y_val.shape

y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
#y_test = to_categorical(y_test)
y_train.shape, y_val.shape

"""**Load the Pretrained Model**"""

pre_trained_model = DenseNet201(input_shape=(192, 256, 3), include_top=False, weights="imagenet")

for layer in pre_trained_model.layers:
    print(layer.name)
    layer.trainable = False
    
print(len(pre_trained_model.layers))

last_layer = pre_trained_model.get_layer('relu')
print('last layer output shape:', last_layer.output_shape)
last_output = last_layer.output

"""**MODEL**"""

# Flatten the output layer to 1 dimension
x = layers.GlobalMaxPooling2D()(last_output)
# Add a fully connected layer with 512 hidden units and ReLU activation
x = layers.Dense(512, activation='relu')(x)
# Add a dropout rate of 0.7
x = layers.Dropout(0.5)(x)
# Add a final sigmoid layer for classification
x = layers.Dense(7, activation='softmax')(x)

# Configure and compile the model

model = Model(pre_trained_model.input, x)
optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)
#model.compile(loss='categorical_crossentropy',
#              optimizer=optimizer,
#              metrics=['accuracy'])

model.compile(loss='categorical_crossentropy', 
              optimizer=optimizer,metrics=["accuracy", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

model.summary()

"""**Training** 

**Feature Extraction**
"""

train_datagen = ImageDataGenerator(rotation_range=60, width_shift_range=0.2, height_shift_range=0.2,
                                   shear_range=0.2, zoom_range=0.2, fill_mode='nearest')

train_datagen.fit(X_train)

val_datagen = ImageDataGenerator()
val_datagen.fit(X_val)

batch_size = 32
epochs = 3
history = model.fit_generator(train_datagen.flow(X_train,y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = val_datagen.flow(X_val, y_val),
                              verbose = 1, steps_per_epoch=(X_train.shape[0] // batch_size), 
                              validation_steps=(X_val.shape[0] // batch_size))

"""**Retraining**"""

for layer in pre_trained_model.layers:
    layer.trainable = True
optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
#model.compile(loss='categorical_crossentropy',
#              optimizer=optimizer,
#              metrics=['accuracy'])

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, 
                                            min_lr=0.000001, cooldown=2)
model.summary()

batch_size = 32
epochs = 20

history = model.fit_generator(train_datagen.flow(X_train,y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = val_datagen.flow(X_val, y_val),
                              verbose = 1, steps_per_epoch=(X_train.shape[0] // batch_size),
                              validation_steps=(X_val.shape[0] // batch_size),
                              callbacks=[learning_rate_reduction])

#loss_val, acc_val = model.evaluate(X_val, y_val, verbose=1)
loss_val, acc_val, prec_val, recall_val = model.evaluate(X_val, y_val, verbose=1)
print("Validation: accuracy = %f  ;  loss_v = %f; prec_val = %f ; recall_val = %f" % (acc_val, loss_val, prec_val, recall_val))

"""testing"""

X_test = np.load("/home/aqsah/projects/isic2018/isic2018/segment/256_192_test.npy")
y_test = np.load("/home/aqsah/projects/isic2018/isic2018/segment/test_labels.npy")
y_test = to_categorical(y_test)
loss_test, acc_test, prec_test, recall_test = model.evaluate(X_test, y_test, verbose=1)
#print("Test: accuracy = %f  ;  loss = %f" % (acc_test, loss_test))
print("Test: accuracy = %f  ;  loss = %f; precision = %f; recall = %f" % (acc_test, loss_test, prec_test, recall_test))


model.save("DenseNetFull.h5")

# Retrieve a list of accuracy results on training and test data
# sets for each training epoch
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Retrieve a list of list results on training and test data
# sets for each training epoch
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Plot training and validation accuracy per epoch
plt.plot(epochs, acc, label = "training")
plt.plot(epochs, val_acc, label = "validation")
plt.legend(loc="upper left")
plt.title('Training and validation accuracy')
plt.savefig("SegmentedDenseNet201Acc2.png")

plt.figure()

# Plot training and validation loss per epoch
plt.plot(epochs, loss, label = "training")
plt.plot(epochs, val_loss, label = "validation")
plt.legend(loc="upper right")
plt.title('Training and validation loss')
plt.savefig("SegmentedDenseNet201Loss2.png")
